{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Classification\n",
    "\n",
    "* A Statistical Model Revisited\n",
    "\n",
    "* Contingency Tables Revisited\n",
    "    * odds\n",
    "    * other hypothesis tests\n",
    "\n",
    "* Logistic Regression\n",
    "    * log odds\n",
    "    * connection to linear regression\n",
    "    * interpretation\n",
    "    * parameters in scikit-learn\n",
    "    * decision function\n",
    "    * precision \n",
    "    * recall\n",
    "    * f1 score\n",
    "    * MCC\n",
    "    * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Statistical Model Revisited\n",
    "\n",
    "Thus far we have looked at statistical models that carry out the regression task.  That is, they take in a set of one or more variables and produce a number.  Specifically, when we say regression we mean:\n",
    "\n",
    "$$ \\hat{y} = mX + b $$\n",
    "\n",
    "On the right hand side:\n",
    "\n",
    "Where `X` is a tensor of one or more variables.  When `X` represents a single variable, we call it a vector.  And when `X` represents more than one variable we typically refer to it as a matrix.  However, it is also possible for `X` to represent higher dimensions.\n",
    "\n",
    "`m` and `b` are just scalars, typically from the real numbers.\n",
    "\n",
    "On the left hand side:\n",
    "\n",
    "$\\hat{y}$ is also typically from the real numbers.  \n",
    "\n",
    "And we say that we regress X on y.\n",
    "\n",
    "One of the important things to note about this procedure is the nature of $\\hat{y}$, because it is from the reals it's output carries distance.  That means:\n",
    "\n",
    "if for a given set of X's $\\hat{y}$ = 5.32 and for another set of X's $\\hat{y}$ = -1.83 then we can say that the output of the first set of variables is strictly higher than the output of the second set.\n",
    "\n",
    "It is not always the case that our output being metrizable, that is being measurable in terms of distance, is useful.  It may be the case that our output should not carry any sense of distance or comparison in anyway.\n",
    "\n",
    "For this we need to introduce a new statistical task, that of classification.\n",
    "\n",
    "## Classification\n",
    "\n",
    "The basic idea behind classification is, what if we output a $\\hat{y}$ that was categorical rather than continuous?  We've already seen categorical variables in the Applying Statistical Tests chapter.  But more formally, a categorical variable is one in which the different classes are just that, classes.  They are just designations.  So let's say we had two classes, A and B.  They could be classes of anything.  Like tall and short people.  Or young and old people.  Or different flavors of ice cream.  As much as people might try to rank order these different classes, neither is truly better than the other.  \n",
    "\n",
    "If you want to try a fun experiment, ask some friends what they think about different classes of things, like maybe whether it's better to be young or old, better to be tall or short, better to eat vanilla or chocolate ice cream.  I bet, as long as your friends aren't too similar, they'll all answer differently.  And that's the point!  There is no objective ordering of any of these classes.  And therefore, we cannot define an explicit metric to rank them.\n",
    "\n",
    "So what?  How are categorical variables useful?  Well turns out they have tons of uses!  We used them extensively in Applying Statistical Tests!  Specifically some of the demographic variables and the converted variable were all categorical.  Without categorical data, we'd never be able to model any of that!  And then we'd be greatly constraining the set of problems we can solve with statistical modeling and analysis.\n",
    "\n",
    "Hopefully I've convinced you that classification is cool!  Now let's look at a basic definition of it, so we can compare against our regression task.\n",
    "\n",
    "### Linear Discriminant Analysis\n",
    "\n",
    "We'll start our analysis of classification by looking at Linear Discriminant Analysis.  This technique was invented by great Ronald Fisher along with many of the other foundations of statistics.\n",
    "\n",
    "Let's start with the problem set up:\n",
    "\n",
    "Assume we have two classes and a bunch of data about the population in general.  The data about the population of interest is referred to as features of the data.  And the two classes are called the labels or target.  \n",
    "\n",
    "To make this practical, let's set up a discrete example:\n",
    "\n",
    "Assume you want to understand whether someone is likely to vote republican or democrat in the up coming election.  Let's assume you have:\n",
    "\n",
    "* Age\n",
    "* Salary\n",
    "* Location\n",
    "\n",
    "Let's first generate the dataset, and then we can start to go over the technique:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "df[\"party\"] = [random.choice([\"republican\", \"democrat\"])\n",
    "               for _ in range(1000)]\n",
    "df[\"Age\"] = np.random.normal(50, 15, size=1000)\n",
    "df[\"Age\"] = df[\"Age\"].astype(int)\n",
    "df[\"Salary\"] = np.random.normal(45000, 1500, size=1000)\n",
    "df[\"Salary\"] = df[\"Salary\"].apply(lambda x: round(x, 2))\n",
    "df[\"Latitude\"] = np.random.normal(39, 15, size=1000)\n",
    "df[\"Latitude\"] = df[\"Latitude\"].apply(lambda x: round(x, 4))\n",
    "df[\"Longitude\"] = np.random.normal(94, 15, size=1000)\n",
    "df[\"Longitude\"] = df[\"Longitude\"].apply(lambda x: round(x, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>party</th>\n",
       "      <th>Age</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>republican</td>\n",
       "      <td>41</td>\n",
       "      <td>46287.29</td>\n",
       "      <td>30.1092</td>\n",
       "      <td>99.0837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>republican</td>\n",
       "      <td>38</td>\n",
       "      <td>45969.98</td>\n",
       "      <td>28.9020</td>\n",
       "      <td>77.9553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>republican</td>\n",
       "      <td>80</td>\n",
       "      <td>45823.78</td>\n",
       "      <td>39.7949</td>\n",
       "      <td>119.1563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>republican</td>\n",
       "      <td>73</td>\n",
       "      <td>43861.54</td>\n",
       "      <td>59.1974</td>\n",
       "      <td>57.3633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>republican</td>\n",
       "      <td>38</td>\n",
       "      <td>48256.24</td>\n",
       "      <td>46.4441</td>\n",
       "      <td>85.7561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        party  Age    Salary  Latitude  Longitude\n",
       "0  republican   41  46287.29   30.1092    99.0837\n",
       "1  republican   38  45969.98   28.9020    77.9553\n",
       "2  republican   80  45823.78   39.7949   119.1563\n",
       "3  republican   73  43861.54   59.1974    57.3633\n",
       "4  republican   38  48256.24   46.4441    85.7561"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we've also generated a target variable, `party`.  This will be what we want our model to predict.  Linear Discriminate Analysis can also be used for dimensionality reduction, which we will look at in a different chapter.\n",
    "\n",
    "For classification the procedure is:\n",
    "\n",
    "1. calculate the mean per class per variable.\n",
    "2. calculate the covariances per class\n",
    "3. apply the least sum of squares algorithm to the two matrices calculated above and take the first component.\n",
    "4. use the diaginal of the dot product between the means and the coefficients to recover the intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mean_per_class(df, target_column):\n",
    "    return df.groupby(target_column).agg(np.mean)\n",
    "\n",
    "def covariance_per_class(df, target_column):\n",
    "    return df.groupby(target_column).agg(np.cov)\n",
    "\n",
    "means = mean_per_class(df, \"party\")\n",
    "covariances = covariance_per_class(df, \"party\")\n",
    "coefficients = np.linalg.lstsq(covariances.values, means.values)[0].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we recover the coefficients!  I decided not to show the intercept because it's a bit complex and doesn't add much in terms of teaching value.  \n",
    "\n",
    "So, we've covered how you train this classifier.  But how do you make predictions?  This is the major difference between classification and regression.  For regression problems you simply apply your matrix to new data and whatever you output is what you get.  With classification the steps are as follows:\n",
    "\n",
    "1. apply a decision function, which will give you back the log likelihood ratio of the positive class.  \n",
    "\n",
    "2. calculate the predicted probabilities for membership to each class are generated from the results of the decision function.\n",
    "\n",
    "3. Get the classes by taking the argmax, which maps to whichever class has a higher probability associated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
