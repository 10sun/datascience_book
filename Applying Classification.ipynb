{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Classification\n",
    "\n",
    "In this chapter we'll look at a few examples of applying classification.  We'll start by looking at A/B testing like we did for applying hypothesis testing.  Then we'll move onto an example with customer churn.  Then we'll look at unbalanced classification.  We'll end the chapter with an example of credit card fraud.  \n",
    "\n",
    "## A/B Testing\n",
    "\n",
    "Recall from the Applying Statistical Tests chapter we want to send an email about a towel sale.  We will have both a control and test group - one in which something about the email changed (test) and one in which the email stays the same as in the past (control).  We will use these two samples to set up an experiment.  Did changing the email effect things?\n",
    "\n",
    "Last time we answered this question with hypothesis testing.  Now we will answer it with a classifier!\n",
    "\n",
    "### Recall Set Up\n",
    "\n",
    "In order to test this question, we can set up an experiment.  Here we will set up a randomized test group and a randomized control group.  \n",
    "\n",
    "The test group will be sent an email, with slightly different copy, or possibly with a picture.  Some specific change will be made, in any event.\n",
    "\n",
    "The control group will get the same email as last time.  This way, we can directly compare, as much as possible between the old email and the new one.  There are many things you typically need to control for, or account for in experimental design.  Some things to account for in this scenario are:\n",
    "\n",
    "1) Age\n",
    "\n",
    "2) Gender\n",
    "\n",
    "3) Location\n",
    "\n",
    "4) Time of Day\n",
    "\n",
    "5) Time of Year\n",
    "\n",
    "6) Approximate Disposable Income\n",
    "\n",
    "\n",
    "## Simulating Some Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    2822\n",
      "0    2178\n",
      "Name: converted, dtype: int64\n",
      "0    4393\n",
      "1     607\n",
      "Name: converted, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def generate_data(df, column, choices, size):\n",
    "    \"\"\"\n",
    "    Generates categorical data given choices.\n",
    "    \n",
    "    Parameters:\n",
    "    * df - pd.DataFrame: the data to add a column to\n",
    "    * column - str: the column to generate\n",
    "    * choices - list: the list of possible choices\n",
    "    \n",
    "    Returns:\n",
    "    A dataframe with the newly generated column.\n",
    "    \"\"\"\n",
    "    df[column] = [random.choice(choices)\n",
    "                  for _ in range(size)]\n",
    "    df = pd.concat([df, pd.get_dummies(df[column])], axis=1)\n",
    "    df = df.drop(column, axis=1)\n",
    "    return df\n",
    "\n",
    "def converted_score(x):\n",
    "    if x[\"male\"] == 1:\n",
    "        gender = 0.7\n",
    "    elif x[\"female\"] == 1:\n",
    "        gender = 1.4\n",
    "    if x[\"white\"] == 1:\n",
    "        race = 0.5\n",
    "    elif x[\"black\"] == 1:\n",
    "        race = 1.4\n",
    "    elif x[\"asian\"] == 1:\n",
    "        race = 2.8\n",
    "    elif x[\"hispanic\"] == 1:\n",
    "        race = 3.7\n",
    "    salary_alpha = gender * race\n",
    "    age_alpha = gender + race\n",
    "    return salary_alpha * x[\"salary\"] + age_alpha * x[\"age\"]\n",
    "\n",
    "def decision_boundary(result):\n",
    "    if result > 250000:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "size = 5000\n",
    "test_df = pd.DataFrame()\n",
    "control_df = pd.DataFrame()\n",
    "gender_choices = [\"male\", \"female\"]\n",
    "race_choices = [\"white\", \"black\", \"asian\", \"hispanic\"]\n",
    "test_salary_mean = 150000\n",
    "test_salary_variance = 30000 \n",
    "control_salary_mean = 55000\n",
    "control_salary_variance = 2000\n",
    "\n",
    "test_df = generate_data(test_df, \"gender\", gender_choices, size)\n",
    "test_df = generate_data(test_df, \"race\", race_choices, size)\n",
    "test_df[\"age\"] = np.random.normal(50, 25, size=len(test_df))\n",
    "test_df[\"age\"] = test_df[\"age\"].astype(int)\n",
    "test_df[\"salary\"] = np.random.normal(test_salary_mean, \n",
    "                                     test_salary_variance, \n",
    "                                     size=len(test_df))\n",
    "test_df[\"salary\"] = test_df[\"salary\"].apply(lambda x: round(x, 2))\n",
    "\n",
    "test_df[\"converted\"] = test_df.apply(converted_score, axis=1)\n",
    "test_df[\"converted\"] = test_df[\"converted\"].apply(decision_boundary)\n",
    "\n",
    "control_df = generate_data(control_df, \"gender\", gender_choices, size)\n",
    "control_df = generate_data(control_df, \"race\", race_choices, size)\n",
    "control_df[\"age\"] = np.random.normal(50, 25, size=len(control_df))\n",
    "control_df[\"age\"] = control_df[\"age\"].astype(int)\n",
    "control_df[\"salary\"] = np.random.normal(control_salary_mean, \n",
    "                                        control_salary_variance, \n",
    "                                        size=len(control_df))\n",
    "control_df[\"salary\"] = control_df[\"salary\"].apply(lambda x: round(x, 2))\n",
    "control_df[\"converted\"] = control_df.apply(converted_score, axis=1)\n",
    "control_df[\"converted\"] = control_df[\"converted\"].apply(decision_boundary)\n",
    "\n",
    "print(test_df[\"converted\"].value_counts())\n",
    "print(control_df[\"converted\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's where things get different! The next step is now to model our test and control and see if the probability of conversion is higher or lower for our test and control sets.  If they are the same or similar then our change likely had little effect.  Of course you should verify this with multiple tests as well as cross validation if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97       562\n",
      "           1       0.96      1.00      0.98       688\n",
      "\n",
      "    accuracy                           0.98      1250\n",
      "   macro avg       0.98      0.97      0.98      1250\n",
      "weighted avg       0.98      0.98      0.98      1250\n",
      "\n",
      "ROC AUC 0.9733096085409252\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "augmented_df = test_df.copy()\n",
    "augmented_y = augmented_df[\"converted\"]\n",
    "cols = augmented_df.columns.tolist()\n",
    "cols.remove(\"converted\")\n",
    "augmented_X = augmented_df[cols]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    augmented_X, augmented_y\n",
    ")\n",
    "test_clf = LogisticRegression(\n",
    "    max_iter=1000, class_weight=\"balanced\", \n",
    "    C=100, penalty=\"l2\", solver=\"liblinear\"\n",
    ")\n",
    "test_clf.fit(X_train, y_train)\n",
    "y_pred = test_clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC\", roc_auc_score(y_test, y_pred))\n",
    "plt.hist(clf.predict_proba(X_test).T[1])\n",
    "plt.title('Did Convert Probability Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1108\n",
      "           1       1.00      1.00      1.00       142\n",
      "\n",
      "    accuracy                           1.00      1250\n",
      "   macro avg       1.00      1.00      1.00      1250\n",
      "weighted avg       1.00      1.00      1.00      1250\n",
      "\n",
      "ROC AUC 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "control_y = control_df[\"converted\"]\n",
    "cols = control_df.columns.tolist()\n",
    "cols.remove(\"converted\")\n",
    "control_X = control_df[cols]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    control_X, control_y\n",
    ")\n",
    "control_clf = LogisticRegression(\n",
    "    max_iter=1000, class_weight=\"balanced\", \n",
    "    C=100, penalty=\"l2\", solver=\"liblinear\"\n",
    ")\n",
    "control_clf.fit(X_train, y_train)\n",
    "y_pred = control_clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC\", roc_auc_score(y_test, y_pred))\n",
    "plt.hist(clf.predict_proba(X_test).T[1])\n",
    "plt.title('Did Convert Probability Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "So we record all of the standard metrics to make sure our model explains what we are seeing well.  Then we look at the probability distributions for convert (y=1) and not convert (y=0).  If\n",
    "\n",
    "$$ P(converted | y=1) \\nsim P(converted | y=0)$$ \n",
    "\n",
    "then we can say our trial produced a meaningful result and we can say that our test data indeed changed something.\n",
    "\n",
    "Note: this is a worked example!  If this was the real world and you saw those accuracy measures, you ought to be very, very skeptical.  Especially the second set.\n",
    "\n",
    "Let's look at the specifics of our example:\n",
    "\n",
    "For test:\n",
    "\n",
    "* about 500 converted with a high probability and 450 converted with a very low probability.\n",
    "\n",
    "For control:\n",
    "\n",
    "* about 1200 people did not convert with a high probability and around 200 converted with a high probability.\n",
    "\n",
    "Clearly the test group had a different experience!  But why?  Let's dig further into the data by looking at `test_df` and `control_df`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>female</th>\n",
       "      <th>male</th>\n",
       "      <th>asian</th>\n",
       "      <th>black</th>\n",
       "      <th>hispanic</th>\n",
       "      <th>white</th>\n",
       "      <th>age</th>\n",
       "      <th>salary</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.503600</td>\n",
       "      <td>0.496400</td>\n",
       "      <td>0.250800</td>\n",
       "      <td>0.258800</td>\n",
       "      <td>0.247800</td>\n",
       "      <td>0.242600</td>\n",
       "      <td>50.339400</td>\n",
       "      <td>150040.746312</td>\n",
       "      <td>0.564400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500037</td>\n",
       "      <td>0.500037</td>\n",
       "      <td>0.433517</td>\n",
       "      <td>0.438019</td>\n",
       "      <td>0.431778</td>\n",
       "      <td>0.428698</td>\n",
       "      <td>24.645724</td>\n",
       "      <td>30019.400205</td>\n",
       "      <td>0.495885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-37.000000</td>\n",
       "      <td>41819.190000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>130438.922500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>150082.165000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>170778.290000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>256016.380000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            female         male        asian        black     hispanic  \\\n",
       "count  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000   \n",
       "mean      0.503600     0.496400     0.250800     0.258800     0.247800   \n",
       "std       0.500037     0.500037     0.433517     0.438019     0.431778   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       1.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       1.000000     1.000000     1.000000     1.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "             white          age         salary    converted  \n",
       "count  5000.000000  5000.000000    5000.000000  5000.000000  \n",
       "mean      0.242600    50.339400  150040.746312     0.564400  \n",
       "std       0.428698    24.645724   30019.400205     0.495885  \n",
       "min       0.000000   -37.000000   41819.190000     0.000000  \n",
       "25%       0.000000    34.000000  130438.922500     0.000000  \n",
       "50%       0.000000    50.000000  150082.165000     1.000000  \n",
       "75%       0.000000    67.000000  170778.290000     1.000000  \n",
       "max       1.000000   143.000000  256016.380000     1.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>female</th>\n",
       "      <th>male</th>\n",
       "      <th>asian</th>\n",
       "      <th>black</th>\n",
       "      <th>hispanic</th>\n",
       "      <th>white</th>\n",
       "      <th>age</th>\n",
       "      <th>salary</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.504400</td>\n",
       "      <td>0.495600</td>\n",
       "      <td>0.264600</td>\n",
       "      <td>0.243800</td>\n",
       "      <td>0.243200</td>\n",
       "      <td>0.248400</td>\n",
       "      <td>49.159800</td>\n",
       "      <td>54987.455054</td>\n",
       "      <td>0.121400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500031</td>\n",
       "      <td>0.500031</td>\n",
       "      <td>0.441164</td>\n",
       "      <td>0.429416</td>\n",
       "      <td>0.429058</td>\n",
       "      <td>0.432128</td>\n",
       "      <td>24.594407</td>\n",
       "      <td>1986.156938</td>\n",
       "      <td>0.326624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-40.000000</td>\n",
       "      <td>48168.270000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>53631.270000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>55009.735000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>56297.607500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>63514.950000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            female         male        asian        black     hispanic  \\\n",
       "count  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000   \n",
       "mean      0.504400     0.495600     0.264600     0.243800     0.243200   \n",
       "std       0.500031     0.500031     0.441164     0.429416     0.429058   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       1.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       1.000000     1.000000     1.000000     0.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "             white          age        salary    converted  \n",
       "count  5000.000000  5000.000000   5000.000000  5000.000000  \n",
       "mean      0.248400    49.159800  54987.455054     0.121400  \n",
       "std       0.432128    24.594407   1986.156938     0.326624  \n",
       "min       0.000000   -40.000000  48168.270000     0.000000  \n",
       "25%       0.000000    32.000000  53631.270000     0.000000  \n",
       "50%       0.000000    49.000000  55009.735000     0.000000  \n",
       "75%       0.000000    66.000000  56297.607500     0.000000  \n",
       "max       1.000000   145.000000  63514.950000     1.000000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a huge difference in salary!  Let's look at how big the difference is on average:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95053.29125800001"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[\"salary\"].mean() - control_df[\"salary\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around 95K!  That's a huge difference in standard of living.  Let's go back to our model and look at our coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "female 0.9292687935837641\n",
      "male -4.103078995398313\n",
      "asian 2.6485300549834054\n",
      "black -2.6993728240235115\n",
      "hispanic 4.453509716475326\n",
      "white -7.576477149249771\n",
      "age -0.0009181427593822856\n",
      "salary 4.08325526503341e-05\n"
     ]
    }
   ],
   "source": [
    "for index, coef in enumerate(test_clf.coef_[0]):\n",
    "    print(test_df.columns[index], coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "female 6.9140512773516924\n",
      "male -7.395307616549721\n",
      "asian -3.8570837201125263\n",
      "black -3.806284792620383\n",
      "hispanic 11.029417213298009\n",
      "white -3.847305039763138\n",
      "age -0.0029246918380587596\n",
      "salary -0.00018044270948364995\n"
     ]
    }
   ],
   "source": [
    "for index, coef in enumerate(control_clf.coef_[0]):\n",
    "    print(control_df.columns[index], coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The big thing to pay attention to here is effect size - how the size of the variable and it's weight changes the decision or result of the model.\n",
    "\n",
    "The effect size of salary may _seem_ small based on these coefficients, but I assure you it's not.  Recall, that for logistic regression the equation is:\n",
    "\n",
    "$$ \\theta_{i}x_{i} $$\n",
    "\n",
    "That means we need to consider the _multiplication_ of these two variables.  Let's just look at the average salary for the two groups to get a sense of effect size:\n",
    "\n",
    "For test:\n",
    "\n",
    "$$ \\theta * mean(salary) = 6.126546673480162 $$\n",
    "\n",
    "That is, `4.08325526503341e-05 * 150040.746312 = 6.126546673480162`.\n",
    "\n",
    "Given that effect size is exponentiated by the base, in this case $e$, the natural number.  We have, an increase in salary by 1 dollar leading to an increase in P(Y=1) of:\n",
    "\n",
    "$$ e^{6.126546673480162} = 457.85231394605876 $$\n",
    "\n",
    "Which is _a lot_.  Especially given the effect size of the other coefficients.\n",
    "\n",
    "I'll leave as an exercise carrying this out for the other model.  But let's just say the effect size is similar.\n",
    "\n",
    "What that implies should be obvious, how much you make matters a ton!  So can we tell if the change actually meant anything?  Nope!  We have no idea.  We should construct this problem again, controlling explicitly for income in order to get a fair test.\n",
    "\n",
    "## Example Two - Customer Churn\n",
    "\n",
    "The next typical data science problem we are going to tackle is customer churn!  This is huge for product development, sales cycles and just keeping a business afloat.  If you know and can predict how much your customers are going to churn you can reliably forecast how much revenue to expect per quarter.  Which is basically essential to any and all businesses.  \n",
    "\n",
    "Since customer churn is so important, it's worth noting that there is more to churn than just what's in your model.  It can help drive decisions, but it's very important to include domain experts in churn conversations.  This means designers, UX and design folks, sales folks, executives and other stakeholders.  All of these folks matter.  For one, your model may not take into account critical variables.  For another thing, you may not be measuring enough.  A good model doesn't replace people, it helps inform a conversation and aids in decision making.\n",
    "\n",
    "Let's start with some context for what Customer Churn is and go through some possible definitions:\n",
    "\n",
    "Customer churn, loosely, is the number of customers that will stop paying for your service over a specified period of time.\n",
    "\n",
    "Definition one:\n",
    "\n",
    "$$ \\frac{customers \\space lost \\space during \\space fixed \\space period}{total \\space customers \\space at \\space the \\space start \\space of \\space the \\space fixed \\space period} $$\n",
    "\n",
    "Notice, this does not take into account the total number of customers gained.  Say for instance you started out with 100 customers, then you gained 1 million over the course of the fixed period, say a month, and then 15 thousand churned.  Your churn for the month would be:\n",
    "\n",
    "$$ \\frac{15000}{100} = 150 \\% $$\n",
    "\n",
    "If 150% of your customers churn, you might think your business is _not_ doing great.  But you gained like a million new customers over the period!  So in actuality, this is very good news, _overall_.  If you don't get a ton of new customers, this might be a good enough metric.\n",
    "\n",
    "Definition two:\n",
    "\n",
    "$$ \\frac{Churn}{Customers_{1} + Customers_{n} / 2} $$\n",
    "\n",
    "Where:\n",
    "\n",
    "$ Customer_{1} $ := number of customers at the start of the month\n",
    "\n",
    "$ Customer_{n} $ := number of customers at the end of the month\n",
    "\n",
    "So over the same window we get:\n",
    "\n",
    "$$ \\frac{15000}{(100 + 1000000)/2} = 0.029\\% $$\n",
    "\n",
    "Which sounds much more reasonable, and accurate.\n",
    "\n",
    "Definition three:\n",
    "\n",
    "$$ \\frac{Churn}{\\frac{1}{n} \\displaystyle \\sum_{i=1}^{n} Customer_{i}} $$\n",
    "\n",
    "Here we take the average of the customer count over the period of interest, this further normalizes the churn.\n",
    "\n",
    "## Modeling Churn\n",
    "\n",
    "Once you have a good measure of Churn, that works for you, the next step is to understand your Churn number.  For this we'll create a model of the world that incorpates other data to understand when and more importantly why customers churn.\n",
    "\n",
    "For this example we will be making use of this dataset from kaggle:\n",
    "\n",
    "https://www.kaggle.com/adammaus/predicting-churn-for-bank-customers\n",
    "\n",
    "To get this part of the notebook to run, you'll need to download and unzip the data locally (a pain I know).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 12 columns):\n",
      "Surname            10000 non-null object\n",
      "CreditScore        10000 non-null int64\n",
      "Geography          10000 non-null object\n",
      "Gender             10000 non-null object\n",
      "Age                10000 non-null int64\n",
      "Tenure             10000 non-null int64\n",
      "Balance            10000 non-null float64\n",
      "NumOfProducts      10000 non-null int64\n",
      "HasCrCard          10000 non-null int64\n",
      "IsActiveMember     10000 non-null int64\n",
      "EstimatedSalary    10000 non-null float64\n",
      "Churn              10000 non-null int64\n",
      "dtypes: float64(2), int64(7), object(3)\n",
      "memory usage: 937.6+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"Churn_Modelling.csv\")\n",
    "df.drop(\"RowNumber\", inplace=True, axis=1)\n",
    "df.drop(\"CustomerId\", inplace=True, axis=1)\n",
    "df[\"Churn\"] = df[\"Exited\"] \n",
    "df.drop(\"Exited\", inplace=True, axis=1)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop the `RowNumber` and the `CustomerId` since they won't be useful for understanding why our customers churn or stay.  Some of the other information may be useful.  First let's look at `Gender`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Churn %</th>\n",
       "      <th>Churn Total</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gender</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Female</th>\n",
       "      <td>0.250715</td>\n",
       "      <td>1139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Male</th>\n",
       "      <td>0.164559</td>\n",
       "      <td>898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Churn %  Churn Total\n",
       "Gender                       \n",
       "Female  0.250715         1139\n",
       "Male    0.164559          898"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary[\"Churn %\"] = df.pivot_table(values=\"Churn\", index='Gender', aggfunc=np.mean)\n",
    "summary[\"Churn Total\"] = df.pivot_table(values=\"Churn\", index=\"Gender\", aggfunc=np.sum)\n",
    "summary.drop(\"Churn\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems pretty clear that `Gender` is going to matter, and that `Churn` is going to be higher for women than men.\n",
    "\n",
    "Now let's see if this is True for all countries under consideration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['France' 'Spain' 'Germany']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Geography</th>\n",
       "      <th>France</th>\n",
       "      <th>Germany</th>\n",
       "      <th>Spain</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gender</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Female</th>\n",
       "      <td>0.203450</td>\n",
       "      <td>0.375524</td>\n",
       "      <td>0.212121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Male</th>\n",
       "      <td>0.127134</td>\n",
       "      <td>0.278116</td>\n",
       "      <td>0.131124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Geography    France   Germany     Spain\n",
       "Gender                                 \n",
       "Female     0.203450  0.375524  0.212121\n",
       "Male       0.127134  0.278116  0.131124"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df[\"Geography\"].unique())\n",
    "summary = df.pivot_table(values=\"Churn\", index=['Gender'], columns=[\"Geography\"], aggfunc=np.mean)\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears churn rates are higher across the board for women over men, however churn rates do vary from country to country:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['France' 'Spain' 'Germany']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Geography</th>\n",
       "      <th>France</th>\n",
       "      <th>Germany</th>\n",
       "      <th>Spain</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gender</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Female</th>\n",
       "      <td>460</td>\n",
       "      <td>448</td>\n",
       "      <td>231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Male</th>\n",
       "      <td>350</td>\n",
       "      <td>366</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Geography  France  Germany  Spain\n",
       "Gender                           \n",
       "Female        460      448    231\n",
       "Male          350      366    182"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df[\"Geography\"].unique())\n",
    "summary = df.pivot_table(values=\"Churn\", index=['Gender'], columns=[\"Geography\"], aggfunc=np.sum)\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is probably a function of population size.  However this means explicitly controlling for `Geography` is probably important, because otherwise we may lose a confounding effect, which would lower the generalizability of our analysis.\n",
    "\n",
    "Next let's look at the effect of age on Churn.  For this we should first run the test for independence followed by a test for correlation.  Recall, we will use Kruskal-Wallis for independence and point bi serial correlation for correlation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KruskalResult(statistic=16030.329492796818, pvalue=0.0)\n",
      "PointbiserialrResult(correlation=0.28532303783506824, pvalue=1.2399313093495365e-186)\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "print(stats.kruskal(df[\"Age\"], df[\"Churn\"]))\n",
    "print(stats.pointbiserialr(df[\"Age\"], df[\"Churn\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a pvalue of zero we reject the null hypothesis that the two variables are independent.  Additionally, we see a pvalue of close to zero for point bi serial correlation, therefore we reject the null hypothesis of no correlation.  So `Age` is a variable of interest.  We can also confirm this with mutual information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05644672])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import feature_selection\n",
    "feature_selection.mutual_info_regression(df[\"Age\"].values.reshape(-1, 1), df[\"Churn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't read An Introduction to Information Theory yet, the important things to note are when mutual information is 1, the variables are perfectly dependent.  When it's zero they are the same.  So there is some weak information sharing between `Age` and `Churn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can actually go through the rest of the variables to see which ones are likely useful for predicting churn with mutual information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NumOfProducts', 0.06494083770576875),\n",
       " ('Age', 0.06393611325910786),\n",
       " ('Balance', 0.016154281264732617),\n",
       " ('IsActiveMember', 0.011668822982624505),\n",
       " ('CreditScore', 0.00460938156808588),\n",
       " ('EstimatedSalary', 0.0027089277988778804),\n",
       " ('Tenure', 0.0024970855875752207),\n",
       " ('HasCrCard', 0.0016902194714116803)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [column for column in df.columns if df[column].dtype != \"object\"]\n",
    "features.remove(\"Churn\")\n",
    "X = df[features]\n",
    "y = df[\"Churn\"]\n",
    "ranks = feature_selection.mutual_info_regression(X, y)\n",
    "rankings = []\n",
    "for index, feature in enumerate(features):\n",
    "    rankings.append((feature, ranks[index]))\n",
    "sorted(rankings, key=lambda t: t[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that `NumOfProducts` is the most important and `HasCrCard` is the least important.  Or rather carries the least information about probability of churning.  This is possibly because most people have a credit card these days.\n",
    "\n",
    "Next let's kick out any variables that don't likely matter.  In this case, we can probably safely remove `HasCrCard`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"HasCrCard\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also drop `Surname`, since any information it gives us is probably spurious:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"Surname\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are almost ready to fit our model!  Let's first get our categorical variables ready:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    if df[column].dtype == \"object\":\n",
    "        df = pd.concat([df, pd.get_dummies(df[column])], axis=1)\n",
    "        df = df.drop(column, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.65      0.74      1944\n",
      "           1       0.35      0.66      0.46       556\n",
      "\n",
      "    accuracy                           0.65      2500\n",
      "   macro avg       0.61      0.66      0.60      2500\n",
      "weighted avg       0.76      0.65      0.68      2500\n",
      "\n",
      "0.6562953341030878\n",
      "11.978264824417066\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score, log_loss\n",
    "\n",
    "y = df[\"Churn\"]\n",
    "cols = df.columns.tolist()\n",
    "cols.remove(\"Churn\")\n",
    "X = df[cols]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=12)\n",
    "logit = LogisticRegression(\n",
    "    C=1, penalty=\"l2\", max_iter=1000, \n",
    "    class_weight=\"balanced\"\n",
    ")\n",
    "logit.fit(X_train, y_train)\n",
    "y_pred = logit.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(roc_auc_score(y_test, y_pred))\n",
    "print(log_loss(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our model doesn't do great, let's try toying around a little bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CreditScore -0.0031436478957918785\n",
      "Age 0.05898746159233117\n",
      "Tenure -0.08679665322930301\n",
      "Balance 3.905155260992167e-06\n",
      "NumOfProducts -0.039819154004494525\n",
      "IsActiveMember -0.07847513122067634\n",
      "EstimatedSalary -9.223377167585139e-07\n",
      "France -0.04440259570810289\n",
      "Germany 0.05012964004326264\n",
      "Spain -0.022347366299728076\n",
      "Female 0.04293418313482472\n",
      "Male -0.059554505098413556\n"
     ]
    }
   ],
   "source": [
    "for index, col in enumerate(cols):\n",
    "    print(col, logit.coef_[0][index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the variables really jump out as the issue.  You might think that `EstimatedSalary` and `Balance` are the issue, but both of them are much larger than the rest, so the size of their coefficients make sense in context of their scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.72      0.80      1944\n",
      "           1       0.42      0.70      0.53       556\n",
      "\n",
      "    accuracy                           0.72      2500\n",
      "   macro avg       0.66      0.71      0.66      2500\n",
      "weighted avg       0.79      0.72      0.74      2500\n",
      "\n",
      "0.7129870177931729\n",
      "9.7262918260128\n"
     ]
    }
   ],
   "source": [
    "logit_linear = LogisticRegression(\n",
    "    solver=\"liblinear\",\n",
    "    C=1, penalty=\"l1\", max_iter=10000, \n",
    "    class_weight=\"balanced\"\n",
    ")\n",
    "logit_linear.fit(X_train, y_train)\n",
    "y_pred = logit_linear.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(roc_auc_score(y_test, y_pred))\n",
    "print(log_loss(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't do \"great\", but not too bad!  We are now in a place to interpret our results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CreditScore -0.0006180222871635596\n",
      "Age 0.0782462365221455\n",
      "Tenure -0.006832107433490348\n",
      "Balance 2.6100726865106033e-06\n",
      "NumOfProducts -0.125957377561036\n",
      "IsActiveMember -0.8492739668684903\n",
      "EstimatedSalary 1.2544822574956195e-07\n",
      "France -0.807417662931865\n",
      "Germany 0.0\n",
      "Spain -0.8244401538543713\n",
      "Female -0.34300710576221705\n",
      "Male -0.9037524239656831\n"
     ]
    }
   ],
   "source": [
    "for index, col in enumerate(cols):\n",
    "    print(col, logit_linear.coef_[0][index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't super telling because of the order of magnitude issue, so let's multiply by our average to at least get a (possibly skewed) sense of scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Age', 3.0454843686676427)\n",
      "('Balance', 0.19963373053408276)\n",
      "('EstimatedSalary', 0.012556143007919497)\n",
      "('Germany', 0.0)\n",
      "('Tenure', -0.03424798814260042)\n",
      "('Female', -0.1558281281477752)\n",
      "('NumOfProducts', -0.1927399791438973)\n",
      "('Spain', -0.20421382610972777)\n",
      "('CreditScore', -0.40204129684176587)\n",
      "('France', -0.40483921619403707)\n",
      "('IsActiveMember', -0.4374610203339594)\n",
      "('Male', -0.49317769775807324)\n"
     ]
    }
   ],
   "source": [
    "features = []\n",
    "for index, col in enumerate(cols):\n",
    "    features.append((col, \n",
    "                     logit_linear.coef_[0][index] * df[col].mean()\n",
    "                    ))\n",
    "features = sorted(features, key=lambda t:t[1], reverse=True)\n",
    "\n",
    "for feature in features:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like `Age`, `Balance` and `EstimatedSalary` are the most important features on the positive contribution to `Churn`.  This is actually really, really bad.  That means that our most valuable customers - those with the highest bank accounts are most likely to churn!  Additionally, looks like there is a negative relationship with `CreditScore` which is a good thing, because that means those who can pay more regularly are more likely to stay customers.  Also, there is a negative relationship with `IsActiveMember`, which makes sense.  This means that those who use the service the least are most likely to churn.  So if someone with a large estimated salary isn't active on the platform, constructing an intervention to keep them around is probably a good idea!\n",
    "\n",
    "Also, it looks like targeting men is a safer bet than targetting women.  But it's probably a good idea to try to change that because you don't want to miss out on 50% on the population!  \n",
    "\n",
    "Let's try rerunning the model with just the most important features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.65      0.75      1944\n",
      "           1       0.36      0.68      0.47       556\n",
      "\n",
      "    accuracy                           0.66      2500\n",
      "   macro avg       0.62      0.67      0.61      2500\n",
      "weighted avg       0.76      0.66      0.69      2500\n",
      "\n",
      "0.6661892708055777\n",
      "11.757214416616833\n"
     ]
    }
   ],
   "source": [
    "y = df[\"Churn\"]\n",
    "cols = [\"Male\", \"IsActiveMember\", \"CreditScore\", \"France\", \"Age\", \"Balance\", \"EstimatedSalary\"]\n",
    "X = df[cols]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=12)\n",
    "logit = LogisticRegression(\n",
    "    C=1, penalty=\"l2\", max_iter=1000, \n",
    "    class_weight=\"balanced\"\n",
    ")\n",
    "logit.fit(X_train, y_train)\n",
    "y_pred = logit.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(roc_auc_score(y_test, y_pred))\n",
    "print(log_loss(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks there was a little bit of shrinkage, but not that much!  Which means this is probably an okay model of what's happening with `Churn`.  That said probably a good idea to watch the demographic and geographic stuff even if it's not super explanatory because it might be subject to change / is important to keep a watch of!\n",
    "\n",
    "## Anamoly Detection\n",
    "\n",
    "It's time for to deal with the problem that is the bane of every data scientists existance - anamoly detection.  If you've never dealt with one of these before, don't worry, at some point you will.  These things come up _a lot_ in information extraction from text.  But they also come up generally in a bunch of places.  Normal models won't work, no matter what you try.  And worse then that, you might not even realize you are dealing with an anamoly detection problem until it's far too late.  As in, you've gone to production and the sample data you saw for training and testing was _far_ too curated.  \n",
    "\n",
    "Anamoly detection, is basically when your data falls into a class around 90-95% and about 10% or less the rest of the time.  It may be the case that this happens because, like I stated above the data you saw when you were developing the model was _not_ representative.  This can get _especially_ bad with multiclass classification where multiple classes are relatively rare in production.  You might say, these cases are easy!  Just ignore the rare classes, no point in dredging the swamp for the extra little bit of value!  And honestly, that might be a good answer, _sometimes_.  But you won't always get lucky with who you are employed by, who your boss is, or what they care about.\n",
    "\n",
    "Or in some cases, it may be by the design of the problem.  Of course, the first scenario is _far_ worse, so we will focus on the second case, where there is an anamolous class, by design.  We will also focus on the case of binary classification, because multiclass classification, is frankly speaking, just too hard.\n",
    "\n",
    "### Problem Set up - Credit Card Fraud Detection\n",
    "\n",
    "The problem of fraud is fairly straight forward - someone uses your card for a purchase that's \"weird\" and the credit card company tells you about it.  Of course, this case isn't going to happen often (we hope).  So it's the minority class by _a lot_.  We might not have many examples per customer of this occurring.  However, we may have 'enough' examples over the course of many customers.  This of course, means there will be a lot more cases of transactions where this _didn't_ happen, which can be tough, because your classifier will just learn the majority class.  This also may not be reflected in your metrics since precision, recall and f1 score will all look _great_.  However, really what this means is, they are great for the majority class, not the minority class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "* https://towardsdatascience.com/hands-on-predict-customer-churn-5c2a42806266\n",
    "* https://blog.markgrowth.com/eliminating-churn-is-growth-hacking-2-0-47a380194a06\n",
    "* https://towardsdatascience.com/churn-prediction-3a4a36c2129a\n",
    "* https://www.profitwell.com/blog/the-complete-saas-guide-to-calculating-churn-rate-and-keeping-it-simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
