{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Natural Language Processing\n",
    "\n",
    "The study of natural language processing is primarily concerned with teaching machines to process and \"understand\" text.  It is fairly straight forward for machines to understand structured data like numbers, however to understand language is far more difficult.  Part of this has to do with the nature of numbers versus the nature of language.  Specifically, words can have multiple meanings in multiple contexts.  While the mean behind a number can change, based on its reference, the definition of a number is consistent across all mediums and contexts.\n",
    "\n",
    "To make this discrete, if you have 5 chairs or 5 starfish, while the type of thing being described changes, the quantity of things being described stays constant.  However, with language we cannot make such a claim.  For instance, let's look at the following sentence:\n",
    "\n",
    "\"I hope you do a great job!\"\n",
    "\n",
    "Taken out of context, its meaning is most likely that the narrator is expressing hope to the subject to do a great job.  Let's see the same sentence in another context:\n",
    "\n",
    "Person1: \"I am going to have the best project\"\n",
    "Person2: \"Yea, right, you know I'll do better\"\n",
    "Person1: \"I hope you do a great job.\"\n",
    "Person1: \"Not\"\n",
    "Person2: \"No need to be sarcastic\"\n",
    "\n",
    "Here \"I hope you do a great job\" is intended _sarcastically_ so the meaning of the statement is reversed, despite us not changing the phrasing of the sentence, the _meaning_ has changed completely.  How do we even begin to encode something like sarcasm in our language?  Would a person even do that in a context we care about?  These are just some of the exciting questions of natural language processing!  \n",
    "\n",
    "Below we'll make our way through the following topics:\n",
    "\n",
    "* Bag Of Words Model\n",
    "* stop words\n",
    "* stemming\n",
    "* lemmatization\n",
    "* part of speech tagging\n",
    "    * n-gram analysis\n",
    "    * Hidden Markov Models\n",
    "\n",
    "* syntax trees\n",
    "* tf-idf\n",
    "* corpus generation\n",
    "* named entity recognition\n",
    "\n",
    "* word2vec\n",
    "* Topic modeling with LDA\n",
    "* Text Classification with Naive Bayes\n",
    "* Text Prediction with Logistic Regression\n",
    "* Building A Simple Recommendation Engine\n",
    "* Fuzzy Matching For Deduplication\n",
    "* Finding Human Traffickers Online\n",
    "\n",
    "References:\n",
    "* https://moj-analytical-services.github.io/NLP-guidance/FeatureSelection.html\n",
    "* https://towardsdatascience.com/feature-selection-on-text-classification-1b86879f548e\n",
    "* https://machinelearningmastery.com/feature-selection-machine-learning-python/\n",
    "* https://nlp.stanford.edu/IR-book/html/htmledition/contents-1.html\n",
    "* http://blog.datumbox.com/using-feature-selection-methods-in-text-classification/\n",
    "* https://www.nltk.org/book/\n",
    "* https://spacy.io/usage/spacy-101/\n",
    "* https://course.spacy.io/\n",
    "* https://pythonspot.com/nltk-stop-words/\n",
    "* https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\n",
    "* https://medium.freecodecamp.org/an-introduction-to-part-of-speech-tagging-and-the-hidden-markov-model-953d45338f24\n",
    "\n",
    "\n",
    "## The Bag Of Words Model\n",
    "\n",
    "To get to a point where we can effectively model language, we need to consider it grounded in some task.  Here our task will always be the same, at a high level, how does a machine understand language?  You'll find the specific tasks that follow are mere consequences of this overarching theme of first, how do we understand language?  And as a secondary question, what aspects of language are useful for our specific understanding?  Therefore, any model, in principal will be interested in segmenting natural language into a decomposed or component state.\n",
    "\n",
    "Let's begin with our first model and a specific sub-task, which will inform what's important.  Here we consider the bag of words model and the task of simple information retrieval.  \n",
    "\n",
    "For this system we will need two parts:\n",
    "\n",
    "* a system for transforming the text into numbers\n",
    "* a system for doing the information retrival\n",
    "\n",
    "Let's first define the text into numbers part of the system - \n",
    "\n",
    "## Enter the Bag Of Words Model\n",
    "\n",
    "The Bag of words model is possibly the simplest model you could think of, let's see some code to implement it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Hello': 1, 'there': 1, 'friends': 1, 'how': 1, 'are': 1, 'you': 1}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "def get_bag_of_words(text):\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    text = regex.sub('', text)\n",
    "    tokens = [elem.rstrip() for elem in text.split(\" \")]\n",
    "    return {token:tokens.count(token) for token in tokens}\n",
    "\n",
    "sentence = \"Hello there friends, how are you?\"\n",
    "get_bag_of_words(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of things to notice here:\n",
    "\n",
    "1. we don't want to keep the punctuation\n",
    "2. We now have a set of numbers that have lost semantic meaning\n",
    "\n",
    "Now let's go about defining our simplicitic informaiton retrevial system.\n",
    "\n",
    "Let's assume that we have a web application that should query something different depending on what a user types in.  We give them a \"search\" bar to look up information.  Let's assume for simplicity, that they can only type in keywords.\n",
    "\n",
    "A good example for this is job search based on keyword.  Here, someone enters a role, like \"data scientist\" and open data science roles are returned.  How could we return the results?  Well the simplest way is with a look up table.  Now we are in a position to set up the second part of the system.  Let's see how that might get coded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('data', 1), ('scientist', 1), ('engineer', 0), ('manager', 0)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Looking for a mid level data scientist'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "def get_bag_of_words(text, space_of_words):\n",
    "    text = text.lower()\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    text = regex.sub('', text)\n",
    "    tokens = [elem.rstrip() for elem in text.split(\" \")]\n",
    "    return [(word,tokens.count(word)) for word in space_of_words]\n",
    "\n",
    "def jobs_to_return(job_phrase):\n",
    "    if job_phrase[0][1] >= 1:\n",
    "        if job_phrase[1][1] >= 1:\n",
    "            return \"Looking for a mid level data scientist\"\n",
    "        elif job_phrase[2][1] >= 1:\n",
    "            return \"Looking for a senior data engineer\"\n",
    "        elif job_phrase[3][1] >= 1:\n",
    "            return \"Looking for an experienced data manager\"\n",
    "    else:\n",
    "        return \"Sorry, we don't have any jobs\"\n",
    "\n",
    "space_of_words = \"data scientist engineer manager\".split()\n",
    "sentence = \"Data Scientist\"\n",
    "job_phrase = get_bag_of_words(sentence, space_of_words)\n",
    "print(job_phrase)\n",
    "jobs_to_return(job_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see here this system is extremely simplistic, but it shows a possible design that could be implemented in the real world - If you have a web connection, you can check out a system I helped with called CALC that uses this very idea:\n",
    "\n",
    "[https://calc.gsa.gov/](https://calc.gsa.gov/)\n",
    "\n",
    "Check it out!\n",
    "\n",
    "So far we've built an incredibly symplistic search engine.  One we could make it more natural, is by allowing for more flexible queries that help humans express what they are after, but that aren't important for our query.\n",
    "\n",
    "## Enter Stop Words\n",
    "\n",
    "First we'll see an example of how to do stop words on their own and then we'll add stop words to our system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hey Fred, I'm looking for new car, do you have any recommendations?\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stop_words(sentence):\n",
    "    stop_words = \"hey i a i'm\".split()\n",
    "    return \" \".join([word \n",
    "                     for word in sentence.split() \n",
    "                     if word not in stop_words])\n",
    "\n",
    "sentence = \"Hey Fred, I'm looking for a new car, do you have any recommendations?\"\n",
    "remove_stop_words(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea here is that we remove words that occur a lot in every day language, but don't hold semantically relevant information.  Most stop word lists are standard and come from analysis of major bodies of text called corpora or corpus in the singular.  From these corpora are massive bodies of text, that are supposed to capture the frequency of language in a general setting.  Of course, domain specific corpora exist as well.  For instance, words in the medical community are likely to have a different frequency and usage than in say the gaming community.  So we can differentiate dialetics and communities, in theory, from the language they use and directly from the frequency and occurence of different types of words.\n",
    "\n",
    "Let's look at a standard set of stop words, from a very popular natural language processing library - Natural Language Toolkit (NLTK):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "me\n",
      "my\n",
      "myself\n",
      "we\n",
      "our\n",
      "ours\n",
      "ourselves\n",
      "you\n",
      "you're\n",
      "you've\n",
      "you'll\n",
      "you'd\n",
      "your\n",
      "yours\n",
      "yourself\n",
      "yourselves\n",
      "he\n",
      "him\n",
      "his\n",
      "himself\n",
      "she\n",
      "she's\n",
      "her\n",
      "hers\n",
      "herself\n",
      "it\n",
      "it's\n",
      "its\n",
      "itself\n",
      "they\n",
      "them\n",
      "their\n",
      "theirs\n",
      "themselves\n",
      "what\n",
      "which\n",
      "who\n",
      "whom\n",
      "this\n",
      "that\n",
      "that'll\n",
      "these\n",
      "those\n",
      "am\n",
      "is\n",
      "are\n",
      "was\n",
      "were\n",
      "be\n",
      "been\n",
      "being\n",
      "have\n",
      "has\n",
      "had\n",
      "having\n",
      "do\n",
      "does\n",
      "did\n",
      "doing\n",
      "a\n",
      "an\n",
      "the\n",
      "and\n",
      "but\n",
      "if\n",
      "or\n",
      "because\n",
      "as\n",
      "until\n",
      "while\n",
      "of\n",
      "at\n",
      "by\n",
      "for\n",
      "with\n",
      "about\n",
      "against\n",
      "between\n",
      "into\n",
      "through\n",
      "during\n",
      "before\n",
      "after\n",
      "above\n",
      "below\n",
      "to\n",
      "from\n",
      "up\n",
      "down\n",
      "in\n",
      "out\n",
      "on\n",
      "off\n",
      "over\n",
      "under\n",
      "again\n",
      "further\n",
      "then\n",
      "once\n",
      "here\n",
      "there\n",
      "when\n",
      "where\n",
      "why\n",
      "how\n",
      "all\n",
      "any\n",
      "both\n",
      "each\n",
      "few\n",
      "more\n",
      "most\n",
      "other\n",
      "some\n",
      "such\n",
      "no\n",
      "nor\n",
      "not\n",
      "only\n",
      "own\n",
      "same\n",
      "so\n",
      "than\n",
      "too\n",
      "very\n",
      "s\n",
      "t\n",
      "can\n",
      "will\n",
      "just\n",
      "don\n",
      "don't\n",
      "should\n",
      "should've\n",
      "now\n",
      "d\n",
      "ll\n",
      "m\n",
      "o\n",
      "re\n",
      "ve\n",
      "y\n",
      "ain\n",
      "aren\n",
      "aren't\n",
      "couldn\n",
      "couldn't\n",
      "didn\n",
      "didn't\n",
      "doesn\n",
      "doesn't\n",
      "hadn\n",
      "hadn't\n",
      "hasn\n",
      "hasn't\n",
      "haven\n",
      "haven't\n",
      "isn\n",
      "isn't\n",
      "ma\n",
      "mightn\n",
      "mightn't\n",
      "mustn\n",
      "mustn't\n",
      "needn\n",
      "needn't\n",
      "shan\n",
      "shan't\n",
      "shouldn\n",
      "shouldn't\n",
      "wasn\n",
      "wasn't\n",
      "weren\n",
      "weren't\n",
      "won\n",
      "won't\n",
      "wouldn\n",
      "wouldn't\n",
      "179\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "for word in stopwords.words(\"english\"):\n",
    "    print(word)\n",
    "print(len(stopwords.words(\"english\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there are 179 words that occur commonly and don't convey meaning we care about for our information retrieval problem, or for some NLP tasks more generally.  Let's make use of the nltk stop words in or problem to see what we get out now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hey Fred, I'm looking new car, recommendations?\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stop_words(sentence):\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    return \" \".join([word \n",
    "                     for word in sentence.split() \n",
    "                     if word not in stop_words])\n",
    "\n",
    "sentence = \"Hey Fred, I'm looking for a new car, do you have any recommendations?\"\n",
    "remove_stop_words(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we get to the crux of what is being asked for and can now move onto more sophisticated processing.  Let's add the stop words component to our job query engine, so we can add more \"natural\" language querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('data', 2), ('scientist', 1), ('engineer', 1), ('manager', 0)]\n",
      "Looking for a mid level data scientist\n",
      "Looking for a senior data engineer\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "def remove_stop_words(sentence):\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    return \" \".join([word \n",
    "                     for word in sentence.split() \n",
    "                     if word not in stop_words])\n",
    "\n",
    "def get_bag_of_words(text, space_of_words):\n",
    "    text = text.lower()\n",
    "    text = remove_stop_words(text)\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    text = regex.sub('', text)\n",
    "    tokens = [elem.rstrip() for elem in text.split(\" \")]\n",
    "    return [(word,tokens.count(word)) for word in space_of_words]\n",
    "    \n",
    "def jobs_to_return(job_phrase):\n",
    "    results = []\n",
    "    if job_phrase[0][1] >= 1:\n",
    "        if job_phrase[1][1] >= 1:\n",
    "            results.append(\"Looking for a mid level data scientist\")\n",
    "        if job_phrase[2][1] >= 1:\n",
    "            results.append(\"Looking for a senior data engineer\")\n",
    "        if job_phrase[3][1] >= 1:\n",
    "            results.append(\"Looking for an experienced data manager\")\n",
    "    else:\n",
    "        results.append(\"Sorry, we don't have any jobs\")\n",
    "    return results\n",
    "\n",
    "space_of_words = \"data scientist engineer manager\".split()\n",
    "sentence = \"I'm looking for a Data Scientist job or a Data Engineer job\"\n",
    "job_phrases = get_bag_of_words(sentence, space_of_words)\n",
    "print(job_phrases)\n",
    "for job in jobs_to_return(job_phrases):\n",
    "    print(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we now have a more flexible search engine because of the preprocessing we've done so far.  However we can pretty easily break our engine if we change our query slightly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('data', 2), ('scientist', 1), ('engineer', 0), ('manager', 0)]\n",
      "Looking for a mid level data scientist\n"
     ]
    }
   ],
   "source": [
    "space_of_words = \"data scientist engineer manager\".split()\n",
    "sentence = \"I'm looking for a Data Scientist job or a Data Engineering job\"\n",
    "job_phrases = get_bag_of_words(sentence, space_of_words)\n",
    "print(job_phrases)\n",
    "for job in jobs_to_return(job_phrases):\n",
    "    print(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened?!  Well it turns out by changing \"data engineer\" to the more natural \"data engineering\", we lose our second search result.  In order to recover it let's introduce our next concept - stemming\n",
    "\n",
    "## Enter Stemming\n",
    "\n",
    "Stemming is the idea of taking the stem of a word.  So in this case, the stem of engineering is engineer.  Stemming will also handle the case of engineer versus engineers.  Basically extra pieces of grammatical syntax are removed.  This is sort of like a form of regularization for words.  Because we create a standard representation for related words that only have slight variance in meaning.\n",
    "\n",
    "Let's look at how we might implement a stemmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I heard you have the hiccups, have you tried jump up and down?'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_stem(word):\n",
    "    if word.endswith(\"s\"):\n",
    "        return word[:-1]\n",
    "    elif word.endswith(\"ing\"):\n",
    "        return word[:-3]\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "sentence = \"I heard you have the hiccups, have you tried jumping up and down?\"\n",
    "\" \".join([get_stem(word) for word in sentence.split(\" \")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we are able to get the stem of the word jumping - in this case jump.  Let's look at another example where our simplistic stemmer fais: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hey!  Are you go runn  later?  I'd love to come with you.\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Hey!  Are you going running  later?  I'd love to come with you.\"\n",
    "\" \".join([get_stem(word) for word in sentence.split(\" \")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, \"runn\" is not the stem of \"running\", so we need a quite sophisticated stemmer to handle all the cases, essentially writing down a lot of grammatical rules and edge cases.  Because that in it of itself would be a large enough project, we won't do that here.  Instead we will make use of an off the shelf stemmer, in this case again from NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run caress fli die mule deni die agre own humbl size meet state siez item sensat tradit refer colon plot\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "def get_stem(word):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    return stemmer.stem(word)\n",
    "\n",
    "plurals = ['running', 'caresses', 'flies', 'dies', 'mules', 'denied',\n",
    "           'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "           'meeting', 'stating', 'siezing', 'itemization',\n",
    "           'sensational', 'traditional', 'reference', 'colonizer',\n",
    "           'plotted']\n",
    "\n",
    "singles = [get_stem(plural) for plural in plurals]\n",
    "print(' '.join(singles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the \"running\" case is now well handled.  Other cases aren't handled perfectly but it's still decent.  Here's an example of our off the shelf stemmer performing well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generous\n"
     ]
    }
   ],
   "source": [
    "print(SnowballStemmer(\"english\").stem(\"generously\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be advised, not all stemmers are created equal, and we can probably always do better by covering more edge cases.  Here is an example of a classic stemmer that doesn't do as well in this case, but is generally pretty good:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gener\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "print(PorterStemmer().stem(\"generously\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, the goal of a good stemmer is that the stemmer gives us the singular case.  Sometimes this is easy, other times, not so much.  So there is always a trade off.  Of course, if we can standardize to some typical expectation for a stem and get the root meaning of the word in question, then it doesn't matter if we cover all the edge cases, of which there will always be an ever expanding list.\n",
    "\n",
    "Let's incorporate our Snowball stemmer into our engine to increase the size of our query surface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for sentence one:\n",
      "Job phrases [('data', 2), ('scientist', 1), ('engin', 1), ('manager', 0)]\n",
      "Looking for a mid level data scientist\n",
      "Looking for a senior data engineer\n",
      "\n",
      "results for sentence two:\n",
      "Job phrases [('data', 2), ('scientist', 1), ('engin', 1), ('manager', 0)]\n",
      "Looking for a mid level data scientist\n",
      "Looking for a senior data engineer\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "def get_stem(word):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    return stemmer.stem(word)\n",
    "\n",
    "def remove_stop_words(sentence):\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    return \" \".join([word \n",
    "                     for word in sentence.split() \n",
    "                     if word not in stop_words])\n",
    "\n",
    "def get_bag_of_words(text, space_of_words):\n",
    "    text = text.lower()\n",
    "    text = remove_stop_words(text)\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    text = regex.sub('', text)\n",
    "    tokens = [elem.rstrip() for elem in text.split(\" \")]\n",
    "    tokens = [get_stem(token) for token in tokens]\n",
    "    return [(word,tokens.count(word)) for word in space_of_words]\n",
    "    \n",
    "def jobs_to_return(job_phrase):\n",
    "    results = []\n",
    "    if job_phrase[0][1] >= 1:\n",
    "        if job_phrase[1][1] >= 1:\n",
    "            results.append(\"Looking for a mid level data scientist\")\n",
    "        if job_phrase[2][1] >= 1:\n",
    "            results.append(\"Looking for a senior data engineer\")\n",
    "        if job_phrase[3][1] >= 1:\n",
    "            results.append(\"Looking for an experienced data manager\")\n",
    "    else:\n",
    "        results.append(\"Sorry, we don't have any jobs\")\n",
    "    return results\n",
    "\n",
    "def process_query(query):\n",
    "    space_of_words = \"data scientist engin manager\".split()\n",
    "    job_phrases = get_bag_of_words(query, space_of_words)\n",
    "    print(\"Job phrases\", job_phrases)\n",
    "    for job in jobs_to_return(job_phrases):\n",
    "        print(job)\n",
    "        \n",
    "\n",
    "sentence_one = \"I'm looking for a Data Scientist job or a Data Engineer job\"\n",
    "print(\"results for sentence one:\")\n",
    "process_query(sentence_one)\n",
    "print()\n",
    "print(\"results for sentence two:\")\n",
    "sentence_two = \"I'm looking for a Data Scientist job or a Data Engineering job\"\n",
    "process_query(sentence_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that now both of our cases work now!  Of course, we had to change our recognized word for \"engineer\" to \"engin\" which is not ideal.  However, this does mean we can cover more cases.  With all things, there is a trade off between specificity and flexibility.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for sentence two:\n",
      "Job phrases [('data', 2), ('scientist', 1), ('engin', 1), ('manager', 0)]\n",
      "Looking for a mid level data scientist\n",
      "Looking for a senior data engineer\n"
     ]
    }
   ],
   "source": [
    "print(\"results for sentence two:\")\n",
    "sentence_two = \"I'm looking for a Data Scientist job or a Data Engine job\"\n",
    "process_query(sentence_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, we possibly get a false positive.  Of course, I don't think \"data engine job\" is a thing, so the only way this comes up is a typo.  \n",
    "\n",
    "What if we wanted even more flexibility in our query?  Well we can take our processing even further with a technique called lemmatization.  Lemmatization is similar to stemming in that, the central component of the word is preserved while all other pieces are disgarded.  The major difference between stemming and lemmatization is stemming is based on a rules engine, while lemmatization makes us of formal theory to find the root of the word.  The stanford nlp group even goes so far as to call lemmatization the \"right way\", while stemming is seen as \"a crude rules engine\".  \n",
    "\n",
    "## Enter Lemmatization\n",
    "\n",
    "Because lemmatization is sophisticated, we won't attempt implementation here, but instead simply make use of NLTK's solution by way of example first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "cactus\n",
      "goose\n",
      "rock\n",
      "python\n",
      "good\n",
      "best\n",
      "run\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize(\"cats\", pos=''))\n",
    "print(lemmatizer.lemmatize(\"cacti\"))\n",
    "print(lemmatizer.lemmatize(\"geese\"))\n",
    "print(lemmatizer.lemmatize(\"rocks\"))\n",
    "print(lemmatizer.lemmatize(\"python\"))\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"run\"))\n",
    "print(lemmatizer.lemmatize(\"run\",'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we can even add a part of speech for increases flexability and to ensure a greater degree of correctness.  Let's go back to our example above and see how our lemmatizer does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running caress fly dy mule denied died agreed owned humbled sized meeting stating siezing itemization sensational traditional reference colonizer plotted\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def get_lemma(word):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return lemmatizer.lemmatize(word)\n",
    "\n",
    "plurals = ['running', 'caresses', 'flies', 'dies', 'mules', 'denied',\n",
    "           'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "           'meeting', 'stating', 'siezing', 'itemization',\n",
    "           'sensational', 'traditional', 'reference', 'colonizer',\n",
    "           'plotted']\n",
    "\n",
    "singles = [get_lemma(plural) for plural in plurals]\n",
    "print(' '.join(singles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, on some of the cases lemmatization doesn't work that well at all, but in other cases, lemmatizing does significantly better.  For example, fly is handled much better with the lemmatizer than the stemmer.  \n",
    "\n",
    "The reason the lemmatizer appears to do not as well, is because it doesn't have the part of speech.  Note that in the first example, we hinted that we'll need part of speech in order to do a good job.  For this we will need to consider some part of speech tagging automatically, or be forced to do this manually ourselves (the horror!!!).\n",
    "\n",
    "Before we move onto explaining part of speech tagging in general, let's look at a motivating example as to why our lemmatizer may be shy about getting the lemmas for our plurals above:\n",
    "\n",
    "`They refuse to permit us to obtain the refuse permit.`\n",
    "\n",
    "In the above sentence, refuse is used twice - once meaning to deny and the second time meaning trash.  The difference in definition is made apparent by the part of speech in use!  So for some other words, the plural maybe the same but without the added context, we can't be sure of the underlying meaning.\n",
    "\n",
    "## Enter Part Of Speech Tagging\n",
    "\n",
    "Part of speech tagging is a wide ranging and increadibly powerful tool.  It's invention is one of the great watershed moments in natural language processing.  With it, we have a basic model of the syntax of natural language and therefore can get a sense of the meaning, via the syntax of the sentence.  \n",
    "\n",
    "Over the years, we've continued to see continual improvement in part of speech tagging because of its central nature to many NLP tasks.  Du"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "def process_word(word):\n",
    "    word = get_stem(word)\n",
    "    return get_lemma(word)\n",
    "\n",
    "def get_stem(word):\n",
    "    stemmer = SnowballStemmer(\"porter\")\n",
    "    return stemmer.stem(word)\n",
    "\n",
    "def get_lemma(word):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return lemmatizer.lemmatize(word)\n",
    "\n",
    "plurals = ['running', 'caresses', 'flies', 'dies', 'mules', 'denied',\n",
    "           'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "           'meeting', 'stating', 'siezing', 'itemization',\n",
    "           'sensational', 'traditional', 'reference', 'colonizer',\n",
    "           'plotted', 'engineering']\n",
    "\n",
    "singles_process = [process_word(plural) for plural in plurals]\n",
    "singles_stem = [get_stem(plural) for plural in plurals]\n",
    "print(singles_process == singles_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fli'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_stem(\"fly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
