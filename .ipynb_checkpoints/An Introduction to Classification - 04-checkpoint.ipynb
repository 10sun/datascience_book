{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Classification\n",
    "\n",
    "* A Statistical Model Revisited\n",
    "* Latent Discriminant Analysis\n",
    "* Quadratic Discriminant Analysis\n",
    "* Logistic Regression\n",
    "    * log odds\n",
    "    * connection to linear regression\n",
    "    * interpretation\n",
    "    * parameters in scikit-learn\n",
    "    * decision function\n",
    "    * precision \n",
    "    * recall\n",
    "    * f1 score\n",
    "    * MCC\n",
    "    * \n",
    "    \n",
    "* Contingency Tables Revisited\n",
    "    * odds\n",
    "    * other hypothesis tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Statistical Model Revisited\n",
    "\n",
    "Thus far we have looked at statistical models that carry out the regression task.  That is, they take in a set of one or more variables and produce a number.  Specifically, when we say regression we mean:\n",
    "\n",
    "$$ \\hat{y} = mX + b $$\n",
    "\n",
    "On the right hand side:\n",
    "\n",
    "Where `X` is a tensor of one or more variables.  When `X` represents a single variable, we call it a vector.  And when `X` represents more than one variable we typically refer to it as a matrix.  However, it is also possible for `X` to represent higher dimensions.\n",
    "\n",
    "`m` and `b` are just scalars, typically from the real numbers.\n",
    "\n",
    "On the left hand side:\n",
    "\n",
    "$\\hat{y}$ is also typically from the real numbers.  \n",
    "\n",
    "And we say that we regress X on y.\n",
    "\n",
    "One of the important things to note about this procedure is the nature of $\\hat{y}$, because it is from the reals it's output carries distance.  That means:\n",
    "\n",
    "if for a given set of X's $\\hat{y}$ = 5.32 and for another set of X's $\\hat{y}$ = -1.83 then we can say that the output of the first set of variables is strictly higher than the output of the second set.\n",
    "\n",
    "It is not always the case that our output being metrizable, that is being measurable in terms of distance, is useful.  It may be the case that our output should not carry any sense of distance or comparison in anyway.\n",
    "\n",
    "For this we need to introduce a new statistical task, that of classification.\n",
    "\n",
    "## Classification\n",
    "\n",
    "The basic idea behind classification is, what if we output a $\\hat{y}$ that was categorical rather than continuous?  We've already seen categorical variables in the Applying Statistical Tests chapter.  But more formally, a categorical variable is one in which the different classes are just that, classes.  They are just designations.  So let's say we had two classes, A and B.  They could be classes of anything.  Like tall and short people.  Or young and old people.  Or different flavors of ice cream.  As much as people might try to rank order these different classes, neither is truly better than the other.  \n",
    "\n",
    "If you want to try a fun experiment, ask some friends what they think about different classes of things, like maybe whether it's better to be young or old, better to be tall or short, better to eat vanilla or chocolate ice cream.  I bet, as long as your friends aren't too similar, they'll all answer differently.  And that's the point!  There is no objective ordering of any of these classes.  And therefore, we cannot define an explicit metric to rank them.\n",
    "\n",
    "So what?  How are categorical variables useful?  Well turns out they have tons of uses!  We used them extensively in Applying Statistical Tests!  Specifically some of the demographic variables and the converted variable were all categorical.  Without categorical data, we'd never be able to model any of that!  And then we'd be greatly constraining the set of problems we can solve with statistical modeling and analysis.\n",
    "\n",
    "Hopefully I've convinced you that classification is cool!  Now let's look at a basic definition of it, so we can compare against our regression task.\n",
    "\n",
    "### Linear Discriminant Analysis\n",
    "\n",
    "We'll start our analysis of classification by looking at Linear Discriminant Analysis.  This technique was invented by great Ronald Fisher along with many of the other foundations of statistics.\n",
    "\n",
    "Let's start with the problem set up:\n",
    "\n",
    "Assume we have two classes and a bunch of data about the population in general.  The data about the population of interest is referred to as features of the data.  And the two classes are called the labels or target.  \n",
    "\n",
    "To make this practical, let's set up a discrete example:\n",
    "\n",
    "Assume you want to understand whether someone is likely to vote republican or democrat in the up coming election.  Let's assume you have:\n",
    "\n",
    "* Age\n",
    "* Salary\n",
    "* Location\n",
    "\n",
    "Let's first generate the dataset, and then we can start to go over the technique:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "df[\"party\"] = [random.choice([\"republican\", \"democrat\"])\n",
    "               for _ in range(2000)]\n",
    "df[\"Age\"] = np.random.normal(50, 15, size=2000)\n",
    "df[\"Age\"] = df[\"Age\"].astype(int)\n",
    "df[\"Salary\"] = np.random.normal(45000, 1500, size=2000)\n",
    "df[\"Salary\"] = df[\"Salary\"].apply(lambda x: round(x, 2))\n",
    "df[\"Latitude\"] = np.random.normal(39, 15, size=2000)\n",
    "df[\"Latitude\"] = df[\"Latitude\"].apply(lambda x: round(x, 4))\n",
    "df[\"Longitude\"] = np.random.normal(94, 15, size=2000)\n",
    "df[\"Longitude\"] = df[\"Longitude\"].apply(lambda x: round(x, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>party</th>\n",
       "      <th>Age</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>republican</td>\n",
       "      <td>42</td>\n",
       "      <td>49100.44</td>\n",
       "      <td>50.4067</td>\n",
       "      <td>112.0127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>republican</td>\n",
       "      <td>53</td>\n",
       "      <td>44189.51</td>\n",
       "      <td>31.2893</td>\n",
       "      <td>97.2973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>republican</td>\n",
       "      <td>48</td>\n",
       "      <td>45943.18</td>\n",
       "      <td>10.4703</td>\n",
       "      <td>84.7361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>republican</td>\n",
       "      <td>46</td>\n",
       "      <td>42621.86</td>\n",
       "      <td>30.6266</td>\n",
       "      <td>94.1107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>republican</td>\n",
       "      <td>53</td>\n",
       "      <td>44060.80</td>\n",
       "      <td>13.4962</td>\n",
       "      <td>74.7860</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        party  Age    Salary  Latitude  Longitude\n",
       "0  republican   42  49100.44   50.4067   112.0127\n",
       "1  republican   53  44189.51   31.2893    97.2973\n",
       "2  republican   48  45943.18   10.4703    84.7361\n",
       "3  republican   46  42621.86   30.6266    94.1107\n",
       "4  republican   53  44060.80   13.4962    74.7860"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we've also generated a target variable, `party`.  This will be what we want our model to predict.  Linear Discriminate Analysis can also be used for dimensionality reduction, which we will look at in a different chapter.\n",
    "\n",
    "For classification the procedure is:\n",
    "\n",
    "1. calculate the mean per class per variable.\n",
    "2. calculate the covariances per class\n",
    "3. apply the least sum of squares algorithm to the two matrices calculated above and take the first component.\n",
    "4. use the diaginal of the dot product between the means and the coefficients to recover the intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mean_per_class(df, target_column):\n",
    "    return df.groupby(target_column).agg(np.mean)\n",
    "\n",
    "def covariance_per_class(df, target_column):\n",
    "    return df.groupby(target_column).agg(np.cov)\n",
    "\n",
    "means = mean_per_class(df, \"party\")\n",
    "covariances = covariance_per_class(df, \"party\")\n",
    "coefficients = np.linalg.lstsq(covariances.values, means.values)[0].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we recover the coefficients!  I decided not to show the intercept because it's a bit complex and doesn't add much in terms of teaching value.  \n",
    "\n",
    "So, we've covered how you train this classifier.  But how do you make predictions?  This is the major difference between classification and regression.  For regression problems you simply apply your matrix to new data and whatever you output is what you get.  With classification the steps are as follows:\n",
    "\n",
    "1. apply a decision function, which will give you back the log likelihood ratio of the positive class.  \n",
    "\n",
    "2. calculate the predicted probabilities for membership to each class are generated from the results of the decision function.\n",
    "\n",
    "3. Get the classes by taking the argmax, which maps to whichever class has a higher probability associated.\n",
    "\n",
    "We won't look at those methods explicitly however it is important to know that this is the general procedure for classification.  Let's turn now to making use of scikit-learn's implementation of Linear Discriminant Analysis for a classification task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.74      0.63       261\n",
      "           1       0.53      0.32      0.40       239\n",
      "\n",
      "    accuracy                           0.54       500\n",
      "   macro avg       0.54      0.53      0.51       500\n",
      "weighted avg       0.54      0.54      0.52       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y = df[\"party\"].map({\"republican\": 0, \"democrat\": 1})\n",
    "X = df[[\"Age\", \"Salary\", \"Longitude\", \"Latitude\"]].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "clf = LinearDiscriminantAnalysis(solver=\"lsqr\")\n",
    "clf.fit(X_train, y_train)\n",
    "y_hat = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Linear Discriminant Analysis is rarely used for classification because it's many assumptions, namely:\n",
    "\n",
    "* Each of the variables is independent \n",
    "* Each of the variables is normally distributed\n",
    "* The covariances of variables of each class must be the same.\n",
    "\n",
    "The third assumption means that the covariance of `Age|republican` must be the same as `Age|democrat`.  An assumption that is rarely met.\n",
    "\n",
    "We can verify this third assumption via Levene's statistical test, which has the null hypothesis:\n",
    "\n",
    "* all inputs have the same variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeveneResult(statistic=0.28586435489564416, pvalue=0.5929425110240991)\n",
      "LeveneResult(statistic=1.0050551737574565, pvalue=0.31621178435344255)\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "republican = df[df[\"party\"] == \"republican\"]\n",
    "democrat = df[df[\"party\"] == \"democrat\"]\n",
    "print(stats.levene(republican[\"Age\"], democrat[\"Age\"], center=\"mean\"))\n",
    "print(stats.levene(republican[\"Salary\"], democrat[\"Salary\"], center=\"mean\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fail to reject the null hypothesis in all cases!  So our assumptions are met!  Now that we are sure of our analysis, we can do something that we can't do with many of our other classifiers, easily interpret the coefficients of our model!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age 0.0027272691522056136\n",
      "Salary -5.358556337285714e-06\n",
      "Longitude 0.0033216839376942753\n",
      "Latitude -0.000390080483058608\n"
     ]
    }
   ],
   "source": [
    "coef = clf.coef_[0]\n",
    "for index, column in enumerate([\"Age\", \"Salary\", \"Longitude\", \"Latitude\"]):\n",
    "    print(column, coef[index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are dealing with binary classification there is only one discriminat function.  We see that `Longitude` has the largest magnitude and therefore dominates the function used to separate the data into one of the two classes.  In fact, we can directly recover the discriminant score function by taking a linear combination of the data and it's weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def get_score_function(clf, x):\n",
    "    coef = clf.coef_[0]\n",
    "    summation = 0\n",
    "    for index, column in enumerate([\"Age\", \"Salary\", \"Longitude\", \"Latitude\"]):\n",
    "        summation += x[column] * coef[index]\n",
    "    return summation\n",
    "\n",
    "get_score = partial(get_score_function, clf)\n",
    "\n",
    "df[\"score\"] = df.apply(get_score, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>party</th>\n",
       "      <th>Age</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>republican</td>\n",
       "      <td>54</td>\n",
       "      <td>47206.99</td>\n",
       "      <td>40.0968</td>\n",
       "      <td>82.0462</td>\n",
       "      <td>0.151202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>democrat</td>\n",
       "      <td>22</td>\n",
       "      <td>46681.90</td>\n",
       "      <td>45.9180</td>\n",
       "      <td>84.8948</td>\n",
       "      <td>0.073934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>republican</td>\n",
       "      <td>42</td>\n",
       "      <td>44900.31</td>\n",
       "      <td>65.8046</td>\n",
       "      <td>83.6835</td>\n",
       "      <td>0.126246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>democrat</td>\n",
       "      <td>62</td>\n",
       "      <td>45391.79</td>\n",
       "      <td>47.5620</td>\n",
       "      <td>74.3915</td>\n",
       "      <td>0.154408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>republican</td>\n",
       "      <td>37</td>\n",
       "      <td>45319.49</td>\n",
       "      <td>50.4515</td>\n",
       "      <td>97.7834</td>\n",
       "      <td>0.163187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        party  Age    Salary  Latitude  Longitude     score\n",
       "0  republican   54  47206.99   40.0968    82.0462  0.151202\n",
       "1    democrat   22  46681.90   45.9180    84.8948  0.073934\n",
       "2  republican   42  44900.31   65.8046    83.6835  0.126246\n",
       "3    democrat   62  45391.79   47.5620    74.3915  0.154408\n",
       "4  republican   37  45319.49   50.4515    97.7834  0.163187"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can think of this score as the value that seperates what the classify uses to decide whether the voter is republican or democrat.\n",
    "\n",
    "Now, just for completeness, let's see what our coefficients would look like with three possible classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "df[\"party\"] = [random.choice([\"republican\", \"democrat\", \"other\"])\n",
    "               for _ in range(2000)]\n",
    "df[\"Age\"] = np.random.normal(50, 15, size=2000)\n",
    "df[\"Age\"] = df[\"Age\"].astype(int)\n",
    "df[\"Salary\"] = np.random.normal(45000, 1500, size=2000)\n",
    "df[\"Salary\"] = df[\"Salary\"].apply(lambda x: round(x, 2))\n",
    "df[\"Latitude\"] = np.random.normal(39, 15, size=2000)\n",
    "df[\"Latitude\"] = df[\"Latitude\"].apply(lambda x: round(x, 4))\n",
    "df[\"Longitude\"] = np.random.normal(94, 15, size=2000)\n",
    "df[\"Longitude\"] = df[\"Longitude\"].apply(lambda x: round(x, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.33      0.31       164\n",
      "           1       0.25      0.18      0.21       148\n",
      "           2       0.40      0.47      0.43       188\n",
      "\n",
      "    accuracy                           0.34       500\n",
      "   macro avg       0.32      0.32      0.32       500\n",
      "weighted avg       0.33      0.34      0.33       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y = df[\"party\"].map({\"republican\": 0, \"democrat\": 1, \"other\": 2})\n",
    "X = df[[\"Age\", \"Salary\", \"Longitude\", \"Latitude\"]].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "clf = LinearDiscriminantAnalysis(solver=\"lsqr\")\n",
    "clf.fit(X_train, y_train)\n",
    "y_hat = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.14169747, 0.02075122, 0.3577216 , 0.19963037],\n",
       "       [0.13783221, 0.02068559, 0.35913213, 0.20505868],\n",
       "       [0.13546089, 0.02070732, 0.36078705, 0.19971238]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, now there are three discriminant functions which are used to differentiate the data.  Each row represents a different linear discriminant function.\n",
    "\n",
    "## Quadratic Discriminant Analysis\n",
    "\n",
    "With Linear Discriminant Analysis we made some very strong assumptions:\n",
    "\n",
    "* Each of the variables is independent \n",
    "* Each of the variables is normally distributed\n",
    "* The covariances of variables of each class must be the same.\n",
    "\n",
    "It is unlikely that the third assumption will be met.  This is the major motivation for Quadratic Discriminant Analysis which drops this assumption.  \n",
    "\n",
    "In Quadratic Discriminant Analysis we therefore drop this assumption.  The other assumptions of Linear Discriminant Analysis persist.  Therefore our assumptions are:\n",
    "\n",
    "* Each of the variables is independent \n",
    "* Each of the variables is normally distributed\n",
    "\n",
    "In addition to relaxing the equivalence of covariances of classes we also now can have of a quadratic nature, hence the name.  Therefore we can fit these new shapes:\n",
    "\n",
    "* conic sections\n",
    "* line\n",
    "* circles\n",
    "* ellipse\n",
    "* parabola\n",
    "* hyperbola\n",
    "\n",
    "Notice that we can still fit a line, meaning that quadratic discriminant analysis has a super set of the curve fitting power of linear discriminant analysis.  One of the use cases I haven't discussed yet, is dimensionality reduction.  It is possible to use linear discriminant analysis to do this.  However, with quadratic discriminant analysis, we can only do classification.  So while quadratic discriminant analysis has many desirable features, it doesn't have the flexability of linear discriminant analysis.  We will cover dimensionality reduction in detail in another chapter, so if you don't know what it is, don't worry!\n",
    "\n",
    "Let's look at what's changed in the implementation of quadratic discriminant analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mean_per_class(df, target_column):\n",
    "    return df.groupby(target_column).agg(np.mean)\n",
    "\n",
    "def covariance_per_class(df, target_column):\n",
    "    return df.groupby(target_column).agg(np.cov)\n",
    "\n",
    "means = mean_per_class(df, \"party\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
