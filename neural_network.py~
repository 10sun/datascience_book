import numpy as np
from sklearn.metrics import mean_squared_error
import pandas as pd
import code

# The neural network framework
class Dense:
    def __init__(self, num_inputs, num_outputs, activation_function):
        # Init all weights between [-1 .. 1].
        # Each input is connected to all outputs.
        # One line per input and one column per output.
        self.weights = np.random.uniform(-1, 1, (num_inputs, num_outputs))
        self.select_activation_function(activation_function)
        
    def tanh(self, x):
        return np.tanh(x)

    def dtanh(self, y):
        return 1 - y ** 2

    def select_activation_function(self, activation_function):
        if activation_function == "tanh":
            self.activation_function = self.tanh
            self.activation_derivative = self.dtanh
            
    def forward(self, X):
        self.output = self.activation_function(X.dot(self.weights))
        return self.output

    def compute_gradient(self, error):
        self.delta = error * self.activation_derivative(self.output)
        # Returns the gradient
        return self.delta.dot(self.weights.T)

    def update_weights(self, X, learning_rate):
        try:
            self.weights += X.T.dot(self.delta) * learning_rate
        except:
            code.interact(local=locals())
            
class Network:
    def __init__(self, layers):
        self.layers = layers

    def forward(self, X):
        output = X
        for layer in self.layers:
            output = layer.forward(output)
        return output

    def backprop(self, X, error, learning_rate):
        # Compute deltas at each layer starting from the last one
        for layer in reversed(self.layers):
            error = layer.compute_gradient(error)

        # Update the weights
        for layer in self.layers:
            layer.update_weights(X, learning_rate)
            X = layer.output

class NeuralNetwork:
    def __init__(self, learning_rate=0.1, target_mse=0.01, epochs=500):
        self.layers = []
        self.network = None
        self.learning_rate = learning_rate
        self.target_mse = target_mse
        self.epochs = epochs
        self.errors = []
        
    def add_layer(self, layer):
        self.layers.append(layer)

    def init_network(self):
        self.network = Network(self.layers)

    def fit(self, X, y):
        self.init_network()
        for epoch in range(self.epochs):
            self.errors = []

            rows, columns = X.shape
            for index in range(rows):
                # Forward
                output = self.network.forward(X[index])

                # Compute the error
                error = y[index] - output
                self.errors.append(error)

                # Back-propagate the error
                self.network.backprop(X[index], error, self.learning_rate)

                mse = (np.array(self.errors) ** 2).mean()
                if mse <= target_mse:
                    break

    def predict(self, X):
        return self.network.forward(X)

def data_generation_easy():
    df = pd.DataFrame()
    for _ in range(1000):
        a = np.random.normal(0, 1)
        b = np.random.normal(0, 3)
        c = np.random.normal(12, 4)
        if a + b + c > 11:
            target = 1
        else:
            target = 0
        df = df.append({
            "A": a,
            "B": b,
            "C": c,
            "target": target
        }, ignore_index=True)
    return df

df = data_generation_easy()
column_names = ["A", "B", "C"]
target_name = "target"
X = df[column_names].values
y = np.array([[elem] for elem in list(df["target"].values)])

nn = NeuralNetwork()
# Dense(inputs, outputs, activation)
nn.add_layer(Dense(3, 20, "tanh"))
nn.add_layer(Dense(20, 20, "tanh"))
nn.add_layer(Dense(20, 20, "tanh"))
nn.add_layer(Dense(20, 1, "tanh"))
nn.fit(X, y)
y_pred = nn.predict(X)
print(mean_squared_error(y_pred, y))
